{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fda71d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:06.086878Z",
     "iopub.status.busy": "2023-12-19T11:36:06.086492Z",
     "iopub.status.idle": "2023-12-19T11:36:11.687956Z",
     "shell.execute_reply": "2023-12-19T11:36:11.687120Z"
    },
    "papermill": {
     "duration": 5.610647,
     "end_time": "2023-12-19T11:36:11.690393",
     "exception": false,
     "start_time": "2023-12-19T11:36:06.079746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc  \n",
    "import os  \n",
    "import time  \n",
    "import warnings \n",
    "from itertools import combinations  \n",
    "from warnings import simplefilter \n",
    "import joblib  \n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "is_offline = False \n",
    "LGB = True\n",
    "NN = False\n",
    "is_train = True  \n",
    "is_infer = True \n",
    "max_lookback = np.nan \n",
    "split_day = 435 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293df70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:11.702271Z",
     "iopub.status.busy": "2023-12-19T11:36:11.701974Z",
     "iopub.status.idle": "2023-12-19T11:36:11.708998Z",
     "shell.execute_reply": "2023-12-19T11:36:11.708143Z"
    },
    "papermill": {
     "duration": 0.014947,
     "end_time": "2023-12-19T11:36:11.710941",
     "exception": false,
     "start_time": "2023-12-19T11:36:11.695994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_average(a):\n",
    "    w = []\n",
    "    n = len(a)\n",
    "    for j in range(1, n + 1):\n",
    "        j = 2 if j == 1 else j\n",
    "        w.append(1 / (2**(n + 1 - j)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75fb60b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:11.721441Z",
     "iopub.status.busy": "2023-12-19T11:36:11.721196Z",
     "iopub.status.idle": "2023-12-19T11:36:11.736373Z",
     "shell.execute_reply": "2023-12-19T11:36:11.735608Z"
    },
    "papermill": {
     "duration": 0.022469,
     "end_time": "2023-12-19T11:36:11.738185",
     "exception": false,
     "start_time": "2023-12-19T11:36:11.715716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a8f1b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:11.748754Z",
     "iopub.status.busy": "2023-12-19T11:36:11.748464Z",
     "iopub.status.idle": "2023-12-19T11:36:11.759967Z",
     "shell.execute_reply": "2023-12-19T11:36:11.759158Z"
    },
    "papermill": {
     "duration": 0.018878,
     "end_time": "2023-12-19T11:36:11.761879",
     "exception": false,
     "start_time": "2023-12-19T11:36:11.743001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc707d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:11.772155Z",
     "iopub.status.busy": "2023-12-19T11:36:11.771896Z",
     "iopub.status.idle": "2023-12-19T11:36:30.250200Z",
     "shell.execute_reply": "2023-12-19T11:36:30.249349Z"
    },
    "papermill": {
     "duration": 18.485848,
     "end_time": "2023-12-19T11:36:30.252394",
     "exception": false,
     "start_time": "2023-12-19T11:36:11.766546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\", \"ask_price\", \"bid_price\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_shape = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a167d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:30.263639Z",
     "iopub.status.busy": "2023-12-19T11:36:30.263333Z",
     "iopub.status.idle": "2023-12-19T11:36:30.946908Z",
     "shell.execute_reply": "2023-12-19T11:36:30.945989Z"
    },
    "papermill": {
     "duration": 0.691503,
     "end_time": "2023-12-19T11:36:30.948979",
     "exception": false,
     "start_time": "2023-12-19T11:36:30.257476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@njit(parallel=True)\\ndef get_bband(data, window=20, num_std_dev=2):\\n    \"\"\"\\n    Calculate Bollinger Bands for each column in the input DataFrame.\\n\\n    Parameters:\\n    - data (numpy.ndarray): Input DataFrame containing price data.\\n    - window (int): Rolling window for Bollinger Bands calculation.\\n    - num_std_dev (int): Number of standard deviations for upper and lower bands.\\n\\n    Returns:\\n    - upper_bands (numpy.ndarray): Upper Bollinger Bands values for each element in the input DataFrame.\\n    - mid_bands (numpy.ndarray): Middle Bollinger Bands (moving average) values for each element in the input DataFrame.\\n    - lower_bands (numpy.ndarray): Lower Bollinger Bands values for each element in the input DataFrame.\\n    \"\"\"\\n    num_rows, num_cols = data.shape\\n    upper_bands = np.zeros_like(data)\\n    lower_bands = np.zeros_like(data)\\n    mid_bands = np.zeros_like(data)\\n\\n    for col in prange(num_cols):\\n        for i in prange(window - 1, num_rows):\\n            window_slice = data[i - window + 1 : i + 1, col]\\n            mid_bands[i, col] = np.mean(window_slice)\\n            std_dev = np.std(window_slice)\\n            upper_bands[i, col] = mid_bands[i, col] + num_std_dev * std_dev\\n            lower_bands[i, col] = mid_bands[i, col] - num_std_dev * std_dev\\n\\n    return upper_bands, mid_bands, lower_bands\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_ema(data, window=14):\n",
    "    \"\"\"\n",
    "    Calculate Exponential Moving Average (EMA) for each column in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input DataFrame containing price data.\n",
    "    - window (int): EMA calculation window.\n",
    "\n",
    "    Returns:\n",
    "    - ema_values (numpy.ndarray): EMA values for each element in the input DataFrame.\n",
    "    \"\"\"\n",
    "    rows, cols = data.shape\n",
    "    ema_values = np.zeros((rows, cols))\n",
    "    alpha = 2 / (window + 1)\n",
    "\n",
    "    for col in prange(cols):\n",
    "        ema_values[window - 1, col] = np.mean(data[:window, col])\n",
    "\n",
    "        for i in prange(window, rows):\n",
    "            ema_values[i, col] = (data[i, col] - ema_values[i - 1, col]) * alpha + ema_values[i - 1, col]\n",
    "            \n",
    "    return ema_values\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_rsi(data, period=14):\n",
    "    \"\"\"\n",
    "    Calculate Relative Strength Index (RSI) for each column in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input DataFrame containing price data.\n",
    "    - period (int): RSI calculation period.\n",
    "\n",
    "    Returns:\n",
    "    - rsi_values (numpy.ndarray): RSI values for each element in the input DataFrame.\n",
    "    \"\"\"\n",
    "    rows, cols = data.shape\n",
    "    rsi_values = np.zeros((rows, cols))\n",
    "\n",
    "    for col in prange(cols):\n",
    "        delta = np.zeros(rows)\n",
    "        delta[1:] = data[1:, col] - data[:-1, col]\n",
    "\n",
    "        gain = np.where(delta > 0, delta, 0)\n",
    "        loss = -np.where(delta < 0, delta, 0)\n",
    "\n",
    "        avg_gain = np.zeros(rows)\n",
    "        avg_loss = np.zeros(rows)\n",
    "\n",
    "        avg_gain[:period] = np.mean(gain[:period])\n",
    "        avg_loss[:period] = np.mean(loss[:period])\n",
    "\n",
    "        for i in prange(period, rows):\n",
    "            avg_gain[i] = (avg_gain[i - 1] * (period - 1) + gain[i]) / period\n",
    "            avg_loss[i] = (avg_loss[i - 1] * (period - 1) + loss[i]) / period\n",
    "\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi_values[:, col] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi_values\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_macd(data, short_window=12, long_window=26, signal_window=9):\n",
    "    \"\"\"\n",
    "    Calculate Moving Average Convergence Divergence (MACD) for each column in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input DataFrame containing price data.\n",
    "    - short_window (int): Short-term EMA window for MACD calculation.\n",
    "    - long_window (int): Long-term EMA window for MACD calculation.\n",
    "    - signal_window (int): Signal line window for MACD calculation.\n",
    "\n",
    "    Returns:\n",
    "    - macd_values (numpy.ndarray): MACD values for each element in the input DataFrame.\n",
    "    - signal_line_values (numpy.ndarray): Signal line values for each element in the input DataFrame.\n",
    "    - histogram_values (numpy.ndarray): MACD histogram values for each element in the input DataFrame.\n",
    "    \"\"\"\n",
    "    rows, cols = data.shape\n",
    "    macd_values = np.zeros((rows, cols))\n",
    "    signal_line_values = np.zeros((rows, cols))\n",
    "    histogram_values = np.zeros((rows, cols))\n",
    "\n",
    "    short_alpha = 2 / (short_window + 1)\n",
    "    long_alpha = 2 / (long_window + 1)\n",
    "    signal_alpha = 2 / (signal_window + 1)\n",
    "\n",
    "    for col in prange(cols):\n",
    "        short_ema = np.zeros(rows)\n",
    "        long_ema = np.zeros(rows)\n",
    "        signal_line = np.zeros(rows)\n",
    "\n",
    "        short_ema[1:] = data[1:, col].copy()\n",
    "        long_ema[1:] = data[1:, col].copy()\n",
    "\n",
    "        for i in prange(1, rows):\n",
    "            short_ema[i] = (data[i, col] - short_ema[i - 1]) * short_alpha + short_ema[i - 1]\n",
    "            long_ema[i] = (data[i, col] - long_ema[i - 1]) * long_alpha + long_ema[i - 1]\n",
    "\n",
    "        macd_values[:, col] = short_ema - long_ema\n",
    "\n",
    "        signal_line[1:] = macd_values[1:, col].copy()\n",
    "\n",
    "        for i in prange(1, rows):\n",
    "            signal_line[i] = (macd_values[i, col] - signal_line[i - 1]) * signal_alpha + signal_line[i - 1]\n",
    "\n",
    "        signal_line_values[:, col] = signal_line\n",
    "        histogram_values[:, col] = macd_values[:, col] - signal_line\n",
    "\n",
    "    return macd_values, signal_line_values, histogram_values\n",
    "'''\n",
    "@njit(parallel=True)\n",
    "def get_bband(data, window=20, num_std_dev=2):\n",
    "    \"\"\"\n",
    "    Calculate Bollinger Bands for each column in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input DataFrame containing price data.\n",
    "    - window (int): Rolling window for Bollinger Bands calculation.\n",
    "    - num_std_dev (int): Number of standard deviations for upper and lower bands.\n",
    "\n",
    "    Returns:\n",
    "    - upper_bands (numpy.ndarray): Upper Bollinger Bands values for each element in the input DataFrame.\n",
    "    - mid_bands (numpy.ndarray): Middle Bollinger Bands (moving average) values for each element in the input DataFrame.\n",
    "    - lower_bands (numpy.ndarray): Lower Bollinger Bands values for each element in the input DataFrame.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = data.shape\n",
    "    upper_bands = np.zeros_like(data)\n",
    "    lower_bands = np.zeros_like(data)\n",
    "    mid_bands = np.zeros_like(data)\n",
    "\n",
    "    for col in prange(num_cols):\n",
    "        for i in prange(window - 1, num_rows):\n",
    "            window_slice = data[i - window + 1 : i + 1, col]\n",
    "            mid_bands[i, col] = np.mean(window_slice)\n",
    "            std_dev = np.std(window_slice)\n",
    "            upper_bands[i, col] = mid_bands[i, col] + num_std_dev * std_dev\n",
    "            lower_bands[i, col] = mid_bands[i, col] - num_std_dev * std_dev\n",
    "\n",
    "    return upper_bands, mid_bands, lower_bands\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd55fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:30.960293Z",
     "iopub.status.busy": "2023-12-19T11:36:30.960008Z",
     "iopub.status.idle": "2023-12-19T11:36:30.988588Z",
     "shell.execute_reply": "2023-12-19T11:36:30.987830Z"
    },
    "papermill": {
     "duration": 0.036536,
     "end_time": "2023-12-19T11:36:30.990513",
     "exception": false,
     "start_time": "2023-12-19T11:36:30.953977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations([\"reference_price\", \"ask_price\", \"bid_price\", \"wap\"], 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "        \n",
    "    for c in [['mid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "        \n",
    "    for _, single_stock_prices_df in tqdm(df.groupby('stock_id')[[\"reference_price\", \"ask_price\", \"bid_price\", \"wap\"]]):\n",
    "        \n",
    "        # RSI\n",
    "        col_rsi = [f'rsi_{col}' for col in single_stock_prices_df.columns]\n",
    "        rsi_values = get_rsi(single_stock_prices_df.values)\n",
    "        df.loc[single_stock_prices_df.index, col_rsi] = rsi_values\n",
    "        \n",
    "        # MACD\n",
    "        macd_values, signal_line_values, histogram_values = get_macd(single_stock_prices_df.values)\n",
    "        col_macd = [f'macd_{col}' for col in single_stock_prices_df.columns]\n",
    "        col_signal = [f'macd_sig_{col}' for col in single_stock_prices_df.columns]\n",
    "        col_hist = [f'macd_hist_{col}' for col in single_stock_prices_df.columns]\n",
    "\n",
    "        df.loc[single_stock_prices_df.index, col_macd] = macd_values\n",
    "        df.loc[single_stock_prices_df.index, col_signal] = signal_line_values\n",
    "        df.loc[single_stock_prices_df.index, col_hist] = histogram_values\n",
    "\n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"group_id\"] = df[\"stock_id\"].map(group_id_mapping)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    \n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    \n",
    "    for window in [3,5,10]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    windows = [3, 5, 10]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "\n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "    \n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60 \n",
    "    df['stage'] = df['minute'].apply(lambda x: 0 if x < 5 else 1)\n",
    "    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n",
    "    \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    df = imbalance_features(df)\n",
    "    gc.collect() \n",
    "    df = other_features(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb814c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:31.001369Z",
     "iopub.status.busy": "2023-12-19T11:36:31.001108Z",
     "iopub.status.idle": "2023-12-19T11:36:31.013368Z",
     "shell.execute_reply": "2023-12-19T11:36:31.012512Z"
    },
    "papermill": {
     "duration": 0.019871,
     "end_time": "2023-12-19T11:36:31.015280",
     "exception": false,
     "start_time": "2023-12-19T11:36:30.995409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a399c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:31.026137Z",
     "iopub.status.busy": "2023-12-19T11:36:31.025858Z",
     "iopub.status.idle": "2023-12-19T11:36:31.035740Z",
     "shell.execute_reply": "2023-12-19T11:36:31.034812Z"
    },
    "papermill": {
     "duration": 0.017521,
     "end_time": "2023-12-19T11:36:31.037748",
     "exception": false,
     "start_time": "2023-12-19T11:36:31.020227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group_id_mapping = [\n",
    "    0, 1, 3, 0, 0, 5, 1, 4, 1, 0, 0, 1, 2, 4, 4, 5, 2, 4, 1, 6, \n",
    "    2, 2, 4, 4, 0, 2, 0, 4, 2, 1, 2, 6, 0, 5, 3, 0, 2, 0, 3, 2, \n",
    "    5, 4, 3, 1, 2, 4, 1, 2, 5, 0, 5, 5, 2, 0, 0, 0, 5, 2, 0, 1, \n",
    "    5, 1, 3, 0, 3, 2, 1, 1, 0, 1, 3, 3, 4, 1, 5, 4, 2, 2, 3, 3, \n",
    "    3, 6, 6, 4, 4, 3, 3, 4, 0, 5, 2, 5, 3, 1, 1, 4, 4, 3, 1, 2, \n",
    "    3, 6, 3, 5, 0, 0, 4, 3, 5, 0, 2, 5, 4, 5, 1, 1, 0, 3, 1, 1, \n",
    "    2, 2, 2, 2, 1, 6, 4, 5, 0, 3, 0, 1, 2, 0, 6, 5, 1, 1, 3, 5, \n",
    "    4, 4, 5, 3, 0, 2, 1, 2, 2, 2, 5, 0, 1, 6, 5, 3, 1, 0, 6, 2, \n",
    "    0, 1, 5, 0, 0, 2, 2, 2, 4, 0, 2, 0, 3, 1, 1, 4, 5, 3, 5, 4, \n",
    "    3, 2, 0, 4, 5, 2, 0, 2, 1, 5, 5, 5, 5, 4, 5, 2, 2, 5, 0, 1]\n",
    "group_id_mapping = {int(k):v for k,v in enumerate(group_id_mapping)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac665b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:31.048568Z",
     "iopub.status.busy": "2023-12-19T11:36:31.048282Z",
     "iopub.status.idle": "2023-12-19T11:36:31.053415Z",
     "shell.execute_reply": "2023-12-19T11:36:31.052615Z"
    },
    "papermill": {
     "duration": 0.013515,
     "end_time": "2023-12-19T11:36:31.056059",
     "exception": false,
     "start_time": "2023-12-19T11:36:31.042544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "if is_offline:\n",
    "    \n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "    \n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31713d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:36:31.066960Z",
     "iopub.status.busy": "2023-12-19T11:36:31.066691Z",
     "iopub.status.idle": "2023-12-19T11:38:23.626534Z",
     "shell.execute_reply": "2023-12-19T11:38:23.625714Z"
    },
    "papermill": {
     "duration": 112.568021,
     "end_time": "2023-12-19T11:38:23.628921",
     "exception": false,
     "start_time": "2023-12-19T11:36:31.060900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:20<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aade9933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:38:23.649761Z",
     "iopub.status.busy": "2023-12-19T11:38:23.649119Z",
     "iopub.status.idle": "2023-12-19T11:48:14.587143Z",
     "shell.execute_reply": "2023-12-19T11:48:14.586075Z"
    },
    "papermill": {
     "duration": 590.950945,
     "end_time": "2023-12-19T11:48:14.589409",
     "exception": false,
     "start_time": "2023-12-19T11:38:23.638464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 21.12it/s] \n",
      "100%|██████████| 200/200 [00:09<00:00, 21.80it/s] \n",
      "100%|██████████| 200/200 [00:01<00:00, 146.62it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.00it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.04it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 144.82it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 144.16it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 152.46it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.99it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 qps: 5.064247941970825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 153.51it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.03it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.82it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.25it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 147.38it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.39it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.84it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 147.36it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.26it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 qps: 4.213846349716187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 148.49it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.16it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.51it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.12it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 143.02it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.85it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.35it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.93it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.29it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 qps: 3.9524752298990884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 141.55it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.81it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.24it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.15it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.13it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.53it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.97it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.71it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.64it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 143.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 qps: 3.801205426454544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 153.70it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.72it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.11it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.60it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 158.08it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.08it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.56it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.70it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.33it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 152.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 qps: 3.7112180042266845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 153.42it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 141.24it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.39it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.13it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 147.43it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.99it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.48it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.30it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.46it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 qps: 3.662972100575765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 148.05it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 152.12it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.85it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.87it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 138.91it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 158.32it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.73it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 145.68it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.86it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 qps: 3.621771080153329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 155.09it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.67it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.70it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.19it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.95it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.60it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.90it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.07it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 147.27it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 qps: 3.5852551251649856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 147.47it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 157.53it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.05it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.71it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.99it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.43it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.26it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.20it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.12it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 qps: 3.5624162753423056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 155.02it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.36it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 152.13it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.37it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.46it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.08it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 142.99it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 157.03it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.53it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 qps: 3.5444720149040223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 155.72it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.34it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.33it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.85it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 134.97it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 152.23it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.84it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 144.95it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 158.63it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 qps: 3.528876839984547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 150.66it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.80it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.46it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.09it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 143.34it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.65it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.61it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.75it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.71it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 qps: 3.5158303479353585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 150.52it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.77it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.02it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.49it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.13it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 143.97it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.77it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.87it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.18it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 qps: 3.504638860775874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 152.45it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.02it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 142.81it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 152.35it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 159.83it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.89it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.60it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 161.14it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.07it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 152.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 qps: 3.4942992499896457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 154.18it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 138.60it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 157.18it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 156.47it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 143.99it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.37it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.76it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.15it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.21it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 146.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 qps: 3.485888498624166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 146.10it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 152.15it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.86it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.40it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.69it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 153.52it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.04it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.75it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 158.14it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 155.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 qps: 3.480410860478878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 151.00it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.66it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 148.71it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 157.15it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will take approximately 3.9844 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "import optiver2023\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "feature_columns = list(df_train_feats.columns)\n",
    "\n",
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps = []\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    models = []\n",
    "    model_save_path = '/kaggle/input/rsi-and-macd/modelitos_para_despues'\n",
    "    for i in range(1, 11):\n",
    "        model_path = os.path.join(model_save_path, f'doblez_{i}.txt')\n",
    "        model = lgb.Booster(model_file=model_path)\n",
    "        models.append(model)\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "        \n",
    "        \n",
    "        lgb_model_weights = weighted_average(models)\n",
    "        lgb_predictions = np.zeros(len(test))\n",
    "        for model, weight in zip(models, lgb_model_weights):\n",
    "            lgb_predictions += weight * model.predict(feat[feature_columns])\n",
    "        \n",
    "        predictions = lgb_predictions\n",
    "        final_predictions = predictions - np.mean(predictions)\n",
    "        clipped_predictions = np.clip(final_predictions, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4191535,
     "sourceId": 7237684,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4188157,
     "sourceId": 7232830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 734.939081,
   "end_time": "2023-12-19T11:48:17.552914",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-19T11:36:02.613833",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
