{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661f30c2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-07T13:15:08.424787Z",
     "iopub.status.busy": "2023-12-07T13:15:08.423968Z",
     "iopub.status.idle": "2023-12-07T13:15:24.919801Z",
     "shell.execute_reply": "2023-12-07T13:15:24.918798Z"
    },
    "papermill": {
     "duration": 16.506747,
     "end_time": "2023-12-07T13:15:24.922098",
     "exception": false,
     "start_time": "2023-12-07T13:15:08.415351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n",
      "/kaggle/input/optiver-trading-at-the-close/train.csv\n",
      "/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n",
      "/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n",
      "/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n",
      "/kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n",
      "/kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n",
      "/kaggle/input/optiver-v1/__results__.html\n",
      "/kaggle/input/optiver-v1/submission.csv\n",
      "/kaggle/input/optiver-v1/base_features.feather\n",
      "/kaggle/input/optiver-v1/__notebook__.ipynb\n",
      "/kaggle/input/optiver-v1/__output__.json\n",
      "/kaggle/input/optiver-v1/kmeans_features.feather\n",
      "/kaggle/input/optiver-v1/custom.css\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from joblib import delayed, Parallel\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from numba import njit, prange\n",
    "import gc\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "DATA_DIR = \"../input\"\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3068fed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:15:24.935703Z",
     "iopub.status.busy": "2023-12-07T13:15:24.935050Z",
     "iopub.status.idle": "2023-12-07T13:15:24.939550Z",
     "shell.execute_reply": "2023-12-07T13:15:24.938683Z"
    },
    "papermill": {
     "duration": 0.013269,
     "end_time": "2023-12-07T13:15:24.941528",
     "exception": false,
     "start_time": "2023-12-07T13:15:24.928259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data configurations\n",
    "USE_PRECOMPUTE_FEATURES = False  # Load precomputed features for train.csv from private dataset (just for speed up)\n",
    "USE_PRECOMPUTE_KMEAN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d45d25",
   "metadata": {
    "papermill": {
     "duration": 0.005497,
     "end_time": "2023-12-07T13:15:24.952790",
     "exception": false,
     "start_time": "2023-12-07T13:15:24.947293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37da67d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:15:24.965133Z",
     "iopub.status.busy": "2023-12-07T13:15:24.964841Z",
     "iopub.status.idle": "2023-12-07T13:15:43.781016Z",
     "shell.execute_reply": "2023-12-07T13:15:43.780202Z"
    },
    "papermill": {
     "duration": 18.824917,
     "end_time": "2023-12-07T13:15:43.783272",
     "exception": false,
     "start_time": "2023-12-07T13:15:24.958355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\n",
    "train_data.dropna(subset=['imbalance_size', 'bid_price', 'ask_price', 'wap'], how='any', inplace=True)\n",
    "train_data['far_price'].fillna(train_data['reference_price'], inplace=True)\n",
    "train_data['near_price'].fillna(train_data['reference_price'], inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aee584",
   "metadata": {
    "papermill": {
     "duration": 0.00557,
     "end_time": "2023-12-07T13:15:43.794995",
     "exception": false,
     "start_time": "2023-12-07T13:15:43.789425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af9405",
   "metadata": {
    "papermill": {
     "duration": 0.005402,
     "end_time": "2023-12-07T13:15:43.805976",
     "exception": false,
     "start_time": "2023-12-07T13:15:43.800574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173bc52d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:15:43.818881Z",
     "iopub.status.busy": "2023-12-07T13:15:43.818586Z",
     "iopub.status.idle": "2023-12-07T13:15:43.935253Z",
     "shell.execute_reply": "2023-12-07T13:15:43.934379Z"
    },
    "papermill": {
     "duration": 0.125706,
     "end_time": "2023-12-07T13:15:43.937367",
     "exception": false,
     "start_time": "2023-12-07T13:15:43.811661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_ema(data, window=14):\n",
    "    \"\"\"\n",
    "    Calculate Exponential Moving Average (EMA) for each column in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input DataFrame containing price data.\n",
    "    - window (int): EMA calculation window.\n",
    "\n",
    "    Returns:\n",
    "    - ema_values (numpy.ndarray): EMA values for each element in the input DataFrame.\n",
    "    \"\"\"\n",
    "    rows, cols = data.shape\n",
    "    ema_values = np.zeros((rows, cols))\n",
    "    alpha = 2 / (window + 1)\n",
    "\n",
    "    for col in prange(cols):\n",
    "        ema_values[window - 1, col] = np.mean(data[:window, col])\n",
    "\n",
    "        for i in prange(window, rows):\n",
    "            ema_values[i, col] = (data[i, col] - ema_values[i - 1, col]) * alpha + ema_values[i - 1, col]\n",
    "            \n",
    "    return ema_values\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_rsi(data, period=14):\n",
    "    \"\"\"\n",
    "    Calculate Relative Strength Index (RSI) for each column in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input DataFrame containing price data.\n",
    "    - period (int): RSI calculation period.\n",
    "\n",
    "    Returns:\n",
    "    - rsi_values (numpy.ndarray): RSI values for each element in the input DataFrame.\n",
    "    \"\"\"\n",
    "    rows, cols = data.shape\n",
    "    rsi_values = np.zeros((rows, cols))\n",
    "\n",
    "    for col in prange(cols):\n",
    "        delta = np.zeros(rows)\n",
    "        delta[1:] = data[1:, col] - data[:-1, col]\n",
    "\n",
    "        gain = np.where(delta > 0, delta, 0)\n",
    "        loss = -np.where(delta < 0, delta, 0)\n",
    "\n",
    "        avg_gain = np.zeros(rows)\n",
    "        avg_loss = np.zeros(rows)\n",
    "\n",
    "        avg_gain[:period] = np.mean(gain[:period])\n",
    "        avg_loss[:period] = np.mean(loss[:period])\n",
    "\n",
    "        for i in prange(period, rows):\n",
    "            avg_gain[i] = (avg_gain[i - 1] * (period - 1) + gain[i]) / period\n",
    "            avg_loss[i] = (avg_loss[i - 1] * (period - 1) + loss[i]) / period\n",
    "\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi_values[:, col] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi_values\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_macd(data, short_window=12, long_window=26, signal_window=9):\n",
    "    \"\"\"\n",
    "    Calculate Moving Average Convergence Divergence (MACD) for each column in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input DataFrame containing price data.\n",
    "    - short_window (int): Short-term EMA window for MACD calculation.\n",
    "    - long_window (int): Long-term EMA window for MACD calculation.\n",
    "    - signal_window (int): Signal line window for MACD calculation.\n",
    "\n",
    "    Returns:\n",
    "    - macd_values (numpy.ndarray): MACD values for each element in the input DataFrame.\n",
    "    - signal_line_values (numpy.ndarray): Signal line values for each element in the input DataFrame.\n",
    "    - histogram_values (numpy.ndarray): MACD histogram values for each element in the input DataFrame.\n",
    "    \"\"\"\n",
    "    rows, cols = data.shape\n",
    "    macd_values = np.zeros((rows, cols))\n",
    "    signal_line_values = np.zeros((rows, cols))\n",
    "    histogram_values = np.zeros((rows, cols))\n",
    "\n",
    "    short_alpha = 2 / (short_window + 1)\n",
    "    long_alpha = 2 / (long_window + 1)\n",
    "    signal_alpha = 2 / (signal_window + 1)\n",
    "\n",
    "    for col in prange(cols):\n",
    "        short_ema = np.zeros(rows)\n",
    "        long_ema = np.zeros(rows)\n",
    "        signal_line = np.zeros(rows)\n",
    "\n",
    "        short_ema[1:] = data[1:, col].copy()\n",
    "        long_ema[1:] = data[1:, col].copy()\n",
    "\n",
    "        for i in prange(1, rows):\n",
    "            short_ema[i] = (data[i, col] - short_ema[i - 1]) * short_alpha + short_ema[i - 1]\n",
    "            long_ema[i] = (data[i, col] - long_ema[i - 1]) * long_alpha + long_ema[i - 1]\n",
    "\n",
    "        macd_values[:, col] = short_ema - long_ema\n",
    "\n",
    "        signal_line[1:] = macd_values[1:, col].copy()\n",
    "\n",
    "        for i in prange(1, rows):\n",
    "            signal_line[i] = (macd_values[i, col] - signal_line[i - 1]) * signal_alpha + signal_line[i - 1]\n",
    "\n",
    "        signal_line_values[:, col] = signal_line\n",
    "        histogram_values[:, col] = macd_values[:, col] - signal_line\n",
    "\n",
    "    return macd_values, signal_line_values, histogram_values\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_bband(data, window=20, num_std_dev=2):\n",
    "    \"\"\"\n",
    "    Calculate Bollinger Bands for each column in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input DataFrame containing price data.\n",
    "    - window (int): Rolling window for Bollinger Bands calculation.\n",
    "    - num_std_dev (int): Number of standard deviations for upper and lower bands.\n",
    "\n",
    "    Returns:\n",
    "    - upper_bands (numpy.ndarray): Upper Bollinger Bands values for each element in the input DataFrame.\n",
    "    - mid_bands (numpy.ndarray): Middle Bollinger Bands (moving average) values for each element in the input DataFrame.\n",
    "    - lower_bands (numpy.ndarray): Lower Bollinger Bands values for each element in the input DataFrame.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = data.shape\n",
    "    upper_bands = np.zeros_like(data)\n",
    "    lower_bands = np.zeros_like(data)\n",
    "    mid_bands = np.zeros_like(data)\n",
    "\n",
    "    for col in prange(num_cols):\n",
    "        for i in prange(window - 1, num_rows):\n",
    "            window_slice = data[i - window + 1 : i + 1, col]\n",
    "            mid_bands[i, col] = np.mean(window_slice)\n",
    "            std_dev = np.std(window_slice)\n",
    "            upper_bands[i, col] = mid_bands[i, col] + num_std_dev * std_dev\n",
    "            lower_bands[i, col] = mid_bands[i, col] - num_std_dev * std_dev\n",
    "\n",
    "    return upper_bands, mid_bands, lower_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8ffe79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:15:43.950282Z",
     "iopub.status.busy": "2023-12-07T13:15:43.949709Z",
     "iopub.status.idle": "2023-12-07T13:15:43.963525Z",
     "shell.execute_reply": "2023-12-07T13:15:43.962713Z"
    },
    "papermill": {
     "duration": 0.022244,
     "end_time": "2023-12-07T13:15:43.965371",
     "exception": false,
     "start_time": "2023-12-07T13:15:43.943127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_base_features(df):\n",
    "    \"\"\"\n",
    "    Generate features dataframes and merges with base df\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): Base dataframe\n",
    "        \n",
    "    Returns:\n",
    "    - DataFrame with base features\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------------------ Technical Analysis and Log returns  ------------------------------\n",
    "    \n",
    "    prices = ['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'wap']\n",
    "\n",
    "    for _, single_stock_prices_df in tqdm(df.groupby('stock_id')[prices]):\n",
    "        # EMA\n",
    "        col_ema = [f'ema_{col}' for col in single_stock_prices_df.columns]\n",
    "        ema_values = get_ema(single_stock_prices_df.values)\n",
    "        df.loc[single_stock_prices_df.index, col_ema] = ema_values\n",
    "\n",
    "        # RSI\n",
    "        col_rsi = [f'rsi_{col}' for col in single_stock_prices_df.columns]\n",
    "        rsi_values = get_rsi(single_stock_prices_df.values)\n",
    "        df.loc[single_stock_prices_df.index, col_rsi] = rsi_values\n",
    "\n",
    "        # MACD\n",
    "        macd_values, signal_line_values, histogram_values = get_macd(single_stock_prices_df.values)\n",
    "        col_macd = [f'macd_{col}' for col in single_stock_prices_df.columns]\n",
    "        col_signal = [f'macd_sig_{col}' for col in single_stock_prices_df.columns]\n",
    "        col_hist = [f'macd_hist_{col}' for col in single_stock_prices_df.columns]\n",
    "\n",
    "        df.loc[single_stock_prices_df.index, col_macd] = macd_values\n",
    "        df.loc[single_stock_prices_df.index, col_signal] = signal_line_values\n",
    "        df.loc[single_stock_prices_df.index, col_hist] = histogram_values\n",
    "\n",
    "        # Bollinger Bands\n",
    "        bband_upper_values, bband_mid_values, bband_lower_values = get_bband(single_stock_prices_df.values, window=20, num_std_dev=2)\n",
    "        col_bband_upper = [f'bband_upper_{col}' for col in single_stock_prices_df.columns]\n",
    "        col_bband_mid = [f'bband_mid_{col}' for col in single_stock_prices_df.columns]\n",
    "        col_bband_lower = [f'bband_lower_{col}' for col in single_stock_prices_df.columns]\n",
    "\n",
    "        df.loc[single_stock_prices_df.index, col_bband_upper] = bband_upper_values\n",
    "        df.loc[single_stock_prices_df.index, col_bband_mid] = bband_mid_values\n",
    "        df.loc[single_stock_prices_df.index, col_bband_lower] = bband_lower_values\n",
    "        \n",
    "        # Log Returns\n",
    "        for col in prices:\n",
    "            df.loc[single_stock_prices_df.index, f'log_return_{col}'] = np.log(single_stock_prices_df[col]).diff().fillna(0)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    df['state'] =  np.where(df['seconds_in_bucket'] < 300, 0, 1)\n",
    "    df['volume'] = df.eval('(ask_size / ask_price) + (bid_size / bid_price)')\n",
    "    df['mid_price'] = df.eval('(ask_price + bid_price) / 2')\n",
    "    df['price_spread'] = df.eval('ask_price - bid_price')\n",
    "    df['far_near_diff'] = df.eval('(far_price - near_price)')\n",
    "    df['ask_bid_size_imb'] = df.eval('(bid_size - ask_size) / (bid_size + ask_size)')\n",
    "    df['imb_mch_size_imb'] = df.eval('(imbalance_size - matched_size)/(imbalance_size + matched_size)')\n",
    "   \n",
    "    features = [c for c in df.columns if c not in [\"row_id\", \"target\"]]\n",
    "    \n",
    "    gc.collect()\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf32630",
   "metadata": {
    "papermill": {
     "duration": 0.005366,
     "end_time": "2023-12-07T13:15:43.976387",
     "exception": false,
     "start_time": "2023-12-07T13:15:43.971021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## K-means Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acac3b9",
   "metadata": {
    "papermill": {
     "duration": 0.005332,
     "end_time": "2023-12-07T13:15:43.987242",
     "exception": false,
     "start_time": "2023-12-07T13:15:43.981910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then we use K-means clustering based on the correlation coefficient of WAP changes between stocks to divide stocks into categories. Once we have these clusters, calculating the mean of each feature within each cluster to generate new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdcb614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:15:43.999954Z",
     "iopub.status.busy": "2023-12-07T13:15:43.999662Z",
     "iopub.status.idle": "2023-12-07T13:15:44.014024Z",
     "shell.execute_reply": "2023-12-07T13:15:44.013160Z"
    },
    "papermill": {
     "duration": 0.022917,
     "end_time": "2023-12-07T13:15:44.015884",
     "exception": false,
     "start_time": "2023-12-07T13:15:43.992967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_kmeans_features(base_feature_df):\n",
    "    \"\"\"\n",
    "    Generate kmeans features dataframes and merges with base df\n",
    "    \n",
    "    Parameters:\n",
    "    - base_feature_df (DataFrame): DataFrame with base features\n",
    "        \n",
    "    Returns:\n",
    "    - DataFrame with additional features\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------Create Group ID --------------------------------------\n",
    "    # Load and pivot the training data\n",
    "    train_df = base_feature_df.pivot(index='time_id', columns='stock_id', values='log_return_wap')\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr = train_df.corr()\n",
    "    ids = corr.index\n",
    "\n",
    "    # Perform KMeans clustering to categorize stocks into 7 clusters\n",
    "    kmeans = KMeans(n_clusters=7, random_state=0, n_init=7).fit(corr.values)\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Group stock IDs based on cluster assignments\n",
    "    clustered_stock_ids = [list(ids[cluster_labels == n]) for n in range(7)]\n",
    "\n",
    "    # Create mapping of stock_id to cluster_id\n",
    "    mapping = {} \n",
    "    stock_ids = set(base_feature_df['stock_id'])\n",
    "\n",
    "    for cluster_id, stock_ids in enumerate(clustered_stock_ids):\n",
    "        for stock_id in stock_ids: \n",
    "            mapping[stock_id] = cluster_id\n",
    "\n",
    "    def set_group(row):\n",
    "        stock_id = row[\"stock_id\"]  \n",
    "        if stock_id in mapping:\n",
    "            return mapping[stock_id]\n",
    "        else:\n",
    "            return -1 \n",
    "    \n",
    "    feature_with_groupID_df = base_feature_df.assign(group_id=lambda df: df.apply(set_group, axis=1).copy())\n",
    "    \n",
    "    gc.collect()\n",
    "    # ---------------------------Merge with base features -----------------------------\n",
    "    \n",
    "    kmeans_df = feature_with_groupID_df\n",
    "\n",
    "    # Define features for aggregation\n",
    "    features = ['date_id', 'time_id', 'stock_id', 'group_id',\n",
    "                'imbalance_buy_sell_flag', 'wap', 'far_near_diff',\n",
    "                'ask_bid_size_imb', 'imb_mch_size_imb',\n",
    "                'log_return_wap', 'log_return_bid_price', 'log_return_ask_price']\n",
    "\n",
    "    for stat in ['mean', 'max', 'min', 'std', 'skew']:\n",
    "        # Initialize lists to store aggregated data for training and testing sets\n",
    "        mat_train = []\n",
    "\n",
    "        # Iterate over clusters\n",
    "        for cluster_id, stock_ids in enumerate(clustered_stock_ids):\n",
    "            # Extract data for stocks in the current cluster from the training set\n",
    "            cluster_train_data = feature_with_groupID_df.loc[feature_with_groupID_df['stock_id'].isin(stock_ids)][features]\n",
    "\n",
    "            # Group by time_id and calculate multiple aggregations\n",
    "            cluster_train_data = cluster_train_data.groupby(['time_id']).agg({\n",
    "                'imbalance_buy_sell_flag': stat,\n",
    "                'wap': stat,\n",
    "                'log_return_wap': stat,\n",
    "                'ask_bid_size_imb': stat,\n",
    "                'imb_mch_size_imb': stat,\n",
    "                'far_near_diff': stat,\n",
    "                'log_return_wap': stat, \n",
    "                'log_return_bid_price': stat, \n",
    "                'log_return_ask_price': stat\n",
    "            })\n",
    "\n",
    "            # Assign a unique identifier for the cluster\n",
    "            cluster_train_data.loc[:, 'group_id'] = cluster_id\n",
    "\n",
    "            # Append to the list\n",
    "            mat_train.append(cluster_train_data)\n",
    "            gc.collect()\n",
    "\n",
    "        # Concatenate the aggregated dataframes for training\n",
    "        kmeans_data = pd.concat(mat_train)\n",
    "\n",
    "        # Reduce precision if necessary\n",
    "        kmeans_data = kmeans_data.astype({\n",
    "            col: np.float32 for col in kmeans_data.columns if col not in ['time_id', 'group_id', 'date_id']\n",
    "        })\n",
    "\n",
    "        # Rename columns with 'kmeans_' prefix for columns coming from kmeans_data\n",
    "        column_mapping = {col: f'kmeans_{col}_{stat}' for col in kmeans_data.columns if col not in ['time_id', 'group_id', 'date_id']}\n",
    "        kmeans_data.rename(columns=column_mapping, inplace=True)\n",
    "        kmeans_data = kmeans_data.reset_index()\n",
    "\n",
    "        # Merge the two DataFrames on 'time_id' and 'group_id'\n",
    "        kmeans_df = kmeans_df.merge(kmeans_data, on=['time_id', 'group_id'], how='left')\n",
    "        gc.collect()\n",
    "\n",
    "    return kmeans_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a306cb83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:15:44.028142Z",
     "iopub.status.busy": "2023-12-07T13:15:44.027770Z",
     "iopub.status.idle": "2023-12-07T13:15:44.034383Z",
     "shell.execute_reply": "2023-12-07T13:15:44.033387Z"
    },
    "papermill": {
     "duration": 0.014855,
     "end_time": "2023-12-07T13:15:44.036302",
     "exception": false,
     "start_time": "2023-12-07T13:15:44.021447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_features(train_data, USE_PRECOMPUTE_FEATURES=True, USE_PRECOMPUTE_KMEAN=True):\n",
    "    \"\"\"\n",
    "    Get the dataframe that combines all features\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data (DataFrame): The original dataframe downloaded from csv file\n",
    "    - USE_PRECOMPUTE_FEATURES (bool)\n",
    "    - USE_PRECOMPUTE_KMEAN (bool)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # ----------------------------------------Base features---------------------------------------\n",
    "    # Extract important base features and store them into base_feature_df\n",
    "    if USE_PRECOMPUTE_FEATURES:\n",
    "        base_feature_df = pd.read_feather(\"/kaggle/input/feature-v1/base_features.feather\")\n",
    "    else:\n",
    "        base_feature_df = generate_base_features(train_data)\n",
    "        base_feature_df.to_feather(\"base_features.feather\") \n",
    "        \n",
    "    gc.collect() \n",
    "    \n",
    "    # --------------------------------------KMeans features---------------------------------------\n",
    "    \n",
    "    if USE_PRECOMPUTE_KMEAN:\n",
    "        kmeans_df = pd.read_feather(\"/kaggle/input/optiver-v1/kmeans_features.feather\")\n",
    "    else:\n",
    "        kmeans_df = generate_kmeans_features(base_feature_df)\n",
    "        kmeans_df.to_feather(\"kmeans_features.feather\") \n",
    "        \n",
    "    gc.collect() \n",
    "    return kmeans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58c9906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:15:44.048552Z",
     "iopub.status.busy": "2023-12-07T13:15:44.048314Z",
     "iopub.status.idle": "2023-12-07T13:18:18.749517Z",
     "shell.execute_reply": "2023-12-07T13:18:18.748531Z"
    },
    "papermill": {
     "duration": 154.710043,
     "end_time": "2023-12-07T13:18:18.752003",
     "exception": false,
     "start_time": "2023-12-07T13:15:44.041960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:34<00:00,  5.77it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n",
      "/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>...</th>\n",
       "      <th>kmeans_log_return_bid_price_std</th>\n",
       "      <th>kmeans_log_return_ask_price_std</th>\n",
       "      <th>kmeans_imbalance_buy_sell_flag_skew</th>\n",
       "      <th>kmeans_wap_skew</th>\n",
       "      <th>kmeans_log_return_wap_skew</th>\n",
       "      <th>kmeans_ask_bid_size_imb_skew</th>\n",
       "      <th>kmeans_imb_mch_size_imb_skew</th>\n",
       "      <th>kmeans_far_near_diff_skew</th>\n",
       "      <th>kmeans_log_return_bid_price_skew</th>\n",
       "      <th>kmeans_log_return_ask_price_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3180602.69</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380276.64</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.057484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266765</td>\n",
       "      <td>-0.076977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166603.91</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.280947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064381</td>\n",
       "      <td>0.619109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>302879.87</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.03</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.150177</td>\n",
       "      <td>0.872909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11917682.27</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389745.62</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.057484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266765</td>\n",
       "      <td>-0.076977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>447549.96</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.95</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.057484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266765</td>\n",
       "      <td>-0.076977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237755</th>\n",
       "      <td>195</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>2440722.89</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>28280361.74</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.054388</td>\n",
       "      <td>-0.435793</td>\n",
       "      <td>-0.296564</td>\n",
       "      <td>0.487839</td>\n",
       "      <td>0.748021</td>\n",
       "      <td>-0.591784</td>\n",
       "      <td>1.475623</td>\n",
       "      <td>1.971861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237756</th>\n",
       "      <td>196</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>349510.47</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>9187699.11</td>\n",
       "      <td>1.000129</td>\n",
       "      <td>1.000386</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.054388</td>\n",
       "      <td>-0.435793</td>\n",
       "      <td>-0.296564</td>\n",
       "      <td>0.487839</td>\n",
       "      <td>0.748021</td>\n",
       "      <td>-0.591784</td>\n",
       "      <td>1.475623</td>\n",
       "      <td>1.971861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237757</th>\n",
       "      <td>197</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>12725436.10</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>-0.245740</td>\n",
       "      <td>-1.269168</td>\n",
       "      <td>-0.889086</td>\n",
       "      <td>0.392227</td>\n",
       "      <td>0.964835</td>\n",
       "      <td>0.780605</td>\n",
       "      <td>-1.330022</td>\n",
       "      <td>-0.676966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237758</th>\n",
       "      <td>198</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1000898.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>94773271.05</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.404687</td>\n",
       "      <td>0.279527</td>\n",
       "      <td>-3.608968</td>\n",
       "      <td>0.488096</td>\n",
       "      <td>0.982521</td>\n",
       "      <td>0.478664</td>\n",
       "      <td>-4.117099</td>\n",
       "      <td>-4.271659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237759</th>\n",
       "      <td>199</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1884285.71</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>24073677.32</td>\n",
       "      <td>1.000859</td>\n",
       "      <td>1.001494</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.494819</td>\n",
       "      <td>-1.486875</td>\n",
       "      <td>-0.341750</td>\n",
       "      <td>0.260391</td>\n",
       "      <td>2.784758</td>\n",
       "      <td>0.506540</td>\n",
       "      <td>-1.127046</td>\n",
       "      <td>1.703790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5237760 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "0               0        0                  0      3180602.69   \n",
       "1               1        0                  0       166603.91   \n",
       "2               2        0                  0       302879.87   \n",
       "3               3        0                  0     11917682.27   \n",
       "4               4        0                  0       447549.96   \n",
       "...           ...      ...                ...             ...   \n",
       "5237755       195      480                540      2440722.89   \n",
       "5237756       196      480                540       349510.47   \n",
       "5237757       197      480                540            0.00   \n",
       "5237758       198      480                540      1000898.84   \n",
       "5237759       199      480                540      1884285.71   \n",
       "\n",
       "         imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
       "0                              1         0.999812   13380276.64   0.999812   \n",
       "1                             -1         0.999896    1642214.25   0.999896   \n",
       "2                             -1         0.999561    1819368.03   0.999561   \n",
       "3                             -1         1.000171   18389745.62   1.000171   \n",
       "4                             -1         0.999532   17860614.95   0.999532   \n",
       "...                          ...              ...           ...        ...   \n",
       "5237755                       -1         1.000317   28280361.74   0.999734   \n",
       "5237756                       -1         1.000643    9187699.11   1.000129   \n",
       "5237757                        0         0.995789   12725436.10   0.995789   \n",
       "5237758                        1         0.999210   94773271.05   0.999210   \n",
       "5237759                       -1         1.002129   24073677.32   1.000859   \n",
       "\n",
       "         near_price  bid_price  ...  kmeans_log_return_bid_price_std  \\\n",
       "0          0.999812   0.999812  ...                         0.000000   \n",
       "1          0.999896   0.999896  ...                         0.000000   \n",
       "2          0.999561   0.999403  ...                         0.000000   \n",
       "3          1.000171   0.999999  ...                         0.000000   \n",
       "4          0.999532   0.999394  ...                         0.000000   \n",
       "...             ...        ...  ...                              ...   \n",
       "5237755    0.999734   1.000317  ...                         0.000162   \n",
       "5237756    1.000386   1.000643  ...                         0.000162   \n",
       "5237757    0.995789   0.995789  ...                         0.000189   \n",
       "5237758    0.999210   0.998970  ...                         0.000124   \n",
       "5237759    1.001494   1.002129  ...                         0.000174   \n",
       "\n",
       "         kmeans_log_return_ask_price_std  kmeans_imbalance_buy_sell_flag_skew  \\\n",
       "0                               0.000000                            -0.057484   \n",
       "1                               0.000000                            -0.280947   \n",
       "2                               0.000000                             0.162070   \n",
       "3                               0.000000                            -0.057484   \n",
       "4                               0.000000                            -0.057484   \n",
       "...                                  ...                                  ...   \n",
       "5237755                         0.000147                             0.054388   \n",
       "5237756                         0.000147                             0.054388   \n",
       "5237757                         0.000193                            -0.245740   \n",
       "5237758                         0.000120                             0.404687   \n",
       "5237759                         0.000108                             0.494819   \n",
       "\n",
       "         kmeans_wap_skew  kmeans_log_return_wap_skew  \\\n",
       "0               0.000000                    0.000000   \n",
       "1               0.000000                    0.000000   \n",
       "2               0.000000                    0.000000   \n",
       "3               0.000000                    0.000000   \n",
       "4               0.000000                    0.000000   \n",
       "...                  ...                         ...   \n",
       "5237755        -0.435793                   -0.296564   \n",
       "5237756        -0.435793                   -0.296564   \n",
       "5237757        -1.269168                   -0.889086   \n",
       "5237758         0.279527                   -3.608968   \n",
       "5237759        -1.486875                   -0.341750   \n",
       "\n",
       "         kmeans_ask_bid_size_imb_skew  kmeans_imb_mch_size_imb_skew  \\\n",
       "0                            0.266765                     -0.076977   \n",
       "1                            0.064381                      0.619109   \n",
       "2                           -0.150177                      0.872909   \n",
       "3                            0.266765                     -0.076977   \n",
       "4                            0.266765                     -0.076977   \n",
       "...                               ...                           ...   \n",
       "5237755                      0.487839                      0.748021   \n",
       "5237756                      0.487839                      0.748021   \n",
       "5237757                      0.392227                      0.964835   \n",
       "5237758                      0.488096                      0.982521   \n",
       "5237759                      0.260391                      2.784758   \n",
       "\n",
       "         kmeans_far_near_diff_skew  kmeans_log_return_bid_price_skew  \\\n",
       "0                         0.000000                          0.000000   \n",
       "1                         0.000000                          0.000000   \n",
       "2                         0.000000                          0.000000   \n",
       "3                         0.000000                          0.000000   \n",
       "4                         0.000000                          0.000000   \n",
       "...                            ...                               ...   \n",
       "5237755                  -0.591784                          1.475623   \n",
       "5237756                  -0.591784                          1.475623   \n",
       "5237757                   0.780605                         -1.330022   \n",
       "5237758                   0.478664                         -4.117099   \n",
       "5237759                   0.506540                         -1.127046   \n",
       "\n",
       "         kmeans_log_return_ask_price_skew  \n",
       "0                                0.000000  \n",
       "1                                0.000000  \n",
       "2                                0.000000  \n",
       "3                                0.000000  \n",
       "4                                0.000000  \n",
       "...                                   ...  \n",
       "5237755                          1.971861  \n",
       "5237756                          1.971861  \n",
       "5237757                         -0.676966  \n",
       "5237758                         -4.271659  \n",
       "5237759                          1.703790  \n",
       "\n",
       "[5237760 rows x 117 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = get_all_features(train_data, USE_PRECOMPUTE_FEATURES, USE_PRECOMPUTE_KMEAN)\n",
    "gc.collect()\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a2bd9",
   "metadata": {
    "papermill": {
     "duration": 0.044283,
     "end_time": "2023-12-07T13:18:20.275451",
     "exception": false,
     "start_time": "2023-12-07T13:18:20.231168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Nearest-Neighbor Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8b124f",
   "metadata": {
    "papermill": {
     "duration": 0.047237,
     "end_time": "2023-12-07T13:18:20.366489",
     "exception": false,
     "start_time": "2023-12-07T13:18:20.319252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then we build nearest-neighbor features based on different metrics for both time-id and stock-id pairs in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53646d1c",
   "metadata": {
    "papermill": {
     "duration": 0.046986,
     "end_time": "2023-12-07T13:18:20.458572",
     "exception": false,
     "start_time": "2023-12-07T13:18:20.411586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Class Hierarchy:\n",
    "#### Neighbors Class:\n",
    "1. Represents a general structure for nearest neighbors.\n",
    "2. Generates random neighbors if the metric is 'random', otherwise uses k-Nearest Neighbors (kNN) algorithm.\n",
    "3. Has methods for rearranging feature values and creating aggregated nearest neighbor features.\n",
    "\n",
    "#### TimeIdNeighbors Class (Inherits from Neighbors):\n",
    "1. Specialized version for time-id based nearest neighbors.\n",
    "2. Implements the rearrange_feature_values method to process feature data for time-id pairs.\n",
    "\n",
    "#### StockIdNeighbors Class (Inherits from Neighbors):\n",
    "1. Specialized version for stock-id based nearest neighbors.\n",
    "2. Implements the rearrange_feature_values method to process feature data for stock-id pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e92bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:18:20.550735Z",
     "iopub.status.busy": "2023-12-07T13:18:20.550363Z",
     "iopub.status.idle": "2023-12-07T13:18:20.570447Z",
     "shell.execute_reply": "2023-12-07T13:18:20.569353Z"
    },
    "papermill": {
     "duration": 0.068083,
     "end_time": "2023-12-07T13:18:20.572628",
     "exception": false,
     "start_time": "2023-12-07T13:18:20.504545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Neighbors:\n",
    "    def __init__(self,\n",
    "                name: str,\n",
    "                df: pd.DataFrame,\n",
    "                feature_col: str,\n",
    "                p: float,\n",
    "                metric: str = 'minkowski',\n",
    "                metric_params: Optional[Dict] = None,\n",
    "                exclude_self: bool = True,\n",
    "                n_neighbors_max: int = 80):\n",
    "        \"\"\"\n",
    "        Initialize Neighbors class.\n",
    "\n",
    "        Parameters:\n",
    "        - name: Identifier for the neighbor type.\n",
    "        - df: DataFrame containing feature data.\n",
    "        - feature_col: Name of feature\n",
    "        - p: Parameter for the Minkowski distance metric.\n",
    "        - metric: Type of distance metric to be used.\n",
    "        - metric_params: Additional parameters for the distance metric.\n",
    "        - exclude_self: Flag indicating whether to exclude self from neighbors.\n",
    "        - n_neighbors_max: Maximum number of neighbors to consider.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.name, self.exclude_self, self.p, self.n_neighbors_max, self.metric = name, exclude_self, p, n_neighbors_max, metric\n",
    "        \n",
    "        # Generate pivot dataframe\n",
    "        pivot = self.process_feature_data(df, feature_col)\n",
    "        \n",
    "        # Generate random neighbors or use kNN\n",
    "        if metric == 'random':\n",
    "            self.neighbors = np.random.randint(len(pivot), size=(len(pivot), n_neighbors_max))\n",
    "            \n",
    "        ##I suppose this includes itself. \n",
    "        else:\n",
    "            nn = NearestNeighbors(n_neighbors=n_neighbors_max, p=p, metric=metric, metric_params=metric_params)\n",
    "            nn.fit(pivot)\n",
    "            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "        \n",
    "        # Placeholder for feature-related attributes\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "        \n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        \"\"\"\n",
    "        Placeholder method for rearranging feature values based on neighbors.\n",
    "\n",
    "        Parameters:\n",
    "        - df (DataFrame): DataFrame containing raw data.\n",
    "        - feature_col (str): Name of the feature column to process.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        \"\"\"\n",
    "        Process time-id based nearest neighbor features.\n",
    "\n",
    "        Parameters:\n",
    "        - df (DataFrame): DataFrame containing raw data.\n",
    "        - feature_col (str): Name of the feature column to process.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Process feature data for time-id neighbors\n",
    "        feature_pivot = self.process_feature_data(df, feature_col)\n",
    "        \n",
    "        self.columns, self.index, self.feature_col = list(feature_pivot.columns), list(feature_pivot.index), feature_col\n",
    "        self.feature_values = np.sum(feature_pivot.values[self.neighbors, :], axis=1)\n",
    "    \n",
    "    def process_feature_data(self, df: pd.DataFrame, feature_col: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process feature data for stock-id neighbors.\n",
    "\n",
    "        Parameters:\n",
    "        - df (DataFrame): DataFrame containing raw data.\n",
    "        - feature_col (str): Name of the feature column to process.\n",
    "\n",
    "        Returns:\n",
    "        - Processed feature pivot DataFrame.\n",
    "        \"\"\"\n",
    "        pivot = minmax_scale(df.pivot(index='time_id', \n",
    "                                      columns='stock_id', \n",
    "                                      values=feature_col).fillna(df.pivot(index='time_id', \n",
    "                                                                          columns='stock_id', \n",
    "                                                                          values=feature_col).mean()))\n",
    "        return pd.DataFrame(pivot)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n",
    "    \n",
    "class StockIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        \"\"\"\n",
    "        Process stock-id based nearest neighbor features.\n",
    "\n",
    "        Parameters:\n",
    "        - df (DataFrame): DataFrame containing raw data.\n",
    "        - feature_col (str): Name of the feature column to process.\n",
    "        \"\"\"\n",
    "        # Process feature data for stock-id neighbors\n",
    "        feature_pivot = self.process_feature_data(df, feature_col)\n",
    "        \n",
    "        self.columns, self.index, self.feature_col = list(feature_pivot.columns), list(feature_pivot.index), feature_col\n",
    "        \n",
    "        ## sum or mean??? and index(self.neighbors) will be out of bounds for axis 1 with size 200(feature_pivot)\n",
    "        ##self.feature_values = np.sum(feature_pivot.values[:, self.neighbors], axis=2)\n",
    "        \n",
    "        valid_neighbors = np.minimum(self.neighbors, feature_pivot.shape[1] - 1)\n",
    "        self.feature_values = np.mean(feature_pivot.values[:, valid_neighbors], axis=2)\n",
    "        \n",
    "    def process_feature_data(self, df: pd.DataFrame, feature_col: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process feature data for stock-id neighbors.\n",
    "\n",
    "        Parameters:\n",
    "        - df (DataFrame): DataFrame containing raw data.\n",
    "        - feature_col (str): Name of the feature column to process.\n",
    "\n",
    "        Returns:\n",
    "        - Processed feature pivot DataFrame.\n",
    "        \"\"\"\n",
    "        pivot = minmax_scale(df.pivot(index='stock_id', \n",
    "                                      columns='time_id', \n",
    "                                      values=feature_col).fillna(df.pivot(index='stock_id', \n",
    "                                                                          columns='time_id', \n",
    "                                                                          values=feature_col).mean()))\n",
    "        return pd.DataFrame(pivot)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea98fe2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:18:20.637232Z",
     "iopub.status.busy": "2023-12-07T13:18:20.636868Z",
     "iopub.status.idle": "2023-12-07T13:18:20.644004Z",
     "shell.execute_reply": "2023-12-07T13:18:20.642989Z"
    },
    "papermill": {
     "duration": 0.055407,
     "end_time": "2023-12-07T13:18:20.647053",
     "exception": false,
     "start_time": "2023-12-07T13:18:20.591646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mapping(nn_type, feature, n_neighbors=3, p=2):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    - nn_type (str): 'time'/'stock'\n",
    "    - feature (str)\n",
    "    - n_neighbors (int)\n",
    "    - p (float)\n",
    "    \n",
    "    Returns:\n",
    "    - A dict that mapping stock_id/time_id to the array of their neighbors' index\n",
    "    \"\"\"\n",
    "    \n",
    "    if nn_type == 'time':\n",
    "        nn = TimeIdNeighbors(name=\"time_id_neighbors\", df=features_df, feature_col=feature, p=p, metric=\"minkowski\", n_neighbors_max=n_neighbors)\n",
    "        \n",
    "    elif nn_type == 'stock':\n",
    "        nn = StockIdNeighbors(name=\"stock_id_neighbors\", df=features_df, feature_col=feature, p=p, metric=\"minkowski\", n_neighbors_max=n_neighbors)\n",
    "    \n",
    "    neighbors = nn.neighbors\n",
    "    dst = {}\n",
    "    for i in range(len(neighbors)):\n",
    "        dst[i] = neighbors[i]\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d582b0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T13:18:20.691244Z",
     "iopub.status.busy": "2023-12-07T13:18:20.690936Z",
     "iopub.status.idle": "2023-12-07T13:18:48.448384Z",
     "shell.execute_reply": "2023-12-07T13:18:48.447252Z"
    },
    "papermill": {
     "duration": 27.775541,
     "end_time": "2023-12-07T13:18:48.451246",
     "exception": false,
     "start_time": "2023-12-07T13:18:20.675705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: array([  0, 160, 105]),\n",
       " 1: array([  1, 130, 160]),\n",
       " 2: array([  2, 105, 160]),\n",
       " 3: array([  3, 160, 105]),\n",
       " 4: array([  4, 160, 130]),\n",
       " 5: array([  5,  45, 130]),\n",
       " 6: array([  6, 160, 105]),\n",
       " 7: array([  7, 141, 140]),\n",
       " 8: array([  8, 130, 160]),\n",
       " 9: array([  9, 160, 105]),\n",
       " 10: array([ 10, 160, 105]),\n",
       " 11: array([ 11, 112,  45]),\n",
       " 12: array([ 12, 123, 160]),\n",
       " 13: array([ 13, 141, 140]),\n",
       " 14: array([ 14,  45, 112]),\n",
       " 15: array([ 15, 130, 112]),\n",
       " 16: array([ 16, 160, 105]),\n",
       " 17: array([ 17, 141, 140]),\n",
       " 18: array([ 18, 130, 160]),\n",
       " 19: array([ 19, 160, 105]),\n",
       " 20: array([ 20, 160, 105]),\n",
       " 21: array([ 21, 160, 105]),\n",
       " 22: array([ 22, 183, 140]),\n",
       " 23: array([ 23, 140, 183]),\n",
       " 24: array([ 24, 160, 105]),\n",
       " 25: array([ 25, 160, 105]),\n",
       " 26: array([ 26,  49, 160]),\n",
       " 27: array([ 27,  23, 140]),\n",
       " 28: array([ 28, 105, 160]),\n",
       " 29: array([ 29, 160, 105]),\n",
       " 30: array([ 30, 148, 195]),\n",
       " 31: array([ 31, 105, 160]),\n",
       " 32: array([ 32, 160, 105]),\n",
       " 33: array([ 33, 130, 151]),\n",
       " 34: array([ 34, 160, 123]),\n",
       " 35: array([ 35, 160, 105]),\n",
       " 36: array([ 36, 160, 105]),\n",
       " 37: array([ 37, 160, 105]),\n",
       " 38: array([ 38, 160, 105]),\n",
       " 39: array([ 39, 160, 105]),\n",
       " 40: array([ 40, 130, 112]),\n",
       " 41: array([ 41,  95, 112]),\n",
       " 42: array([ 42, 160, 130]),\n",
       " 43: array([ 43, 160, 105]),\n",
       " 44: array([ 44, 160, 105]),\n",
       " 45: array([ 45, 112, 160]),\n",
       " 46: array([ 46, 105, 160]),\n",
       " 47: array([ 47, 130, 160]),\n",
       " 48: array([ 48, 160,  45]),\n",
       " 49: array([ 49,  26, 160]),\n",
       " 50: array([ 50,  45, 160]),\n",
       " 51: array([ 51,  45, 112]),\n",
       " 52: array([ 52, 160, 105]),\n",
       " 53: array([ 53, 160, 105]),\n",
       " 54: array([54, 45, 75]),\n",
       " 55: array([ 55, 160, 105]),\n",
       " 56: array([ 56, 112,  45]),\n",
       " 57: array([ 57, 160, 105]),\n",
       " 58: array([ 58,  45, 112]),\n",
       " 59: array([ 59, 130, 160]),\n",
       " 60: array([ 60,  45, 112]),\n",
       " 61: array([ 61, 160, 105]),\n",
       " 62: array([ 62, 105, 160]),\n",
       " 63: array([ 63, 160, 105]),\n",
       " 64: array([ 64, 160, 105]),\n",
       " 65: array([ 65, 195, 148]),\n",
       " 66: array([ 66, 198, 160]),\n",
       " 67: array([ 67, 160, 123]),\n",
       " 68: array([ 68, 160, 105]),\n",
       " 69: array([ 69, 105, 160]),\n",
       " 70: array([ 70, 105, 160]),\n",
       " 71: array([ 71, 160, 105]),\n",
       " 72: array([ 72, 140, 183]),\n",
       " 73: array([ 73, 160, 105]),\n",
       " 74: array([ 74, 130, 160]),\n",
       " 75: array([ 75,  45, 112]),\n",
       " 76: array([ 76, 105, 160]),\n",
       " 77: array([ 77, 160, 105]),\n",
       " 78: array([ 78, 160, 105]),\n",
       " 79: array([ 79, 160, 105]),\n",
       " 80: array([ 80, 130, 160]),\n",
       " 81: array([ 81, 160, 105]),\n",
       " 82: array([ 82, 105, 160]),\n",
       " 83: array([ 83, 140, 160]),\n",
       " 84: array([ 84, 175,  45]),\n",
       " 85: array([ 85, 160, 105]),\n",
       " 86: array([ 86, 112,  45]),\n",
       " 87: array([ 87, 140, 183]),\n",
       " 88: array([ 88,  45, 112]),\n",
       " 89: array([ 89, 140, 141]),\n",
       " 90: array([ 90, 160, 151]),\n",
       " 91: array([ 91,  45, 112]),\n",
       " 92: array([ 92, 130, 112]),\n",
       " 93: array([93, 49, 26]),\n",
       " 94: array([ 94, 160, 105]),\n",
       " 95: array([ 95, 112,  41]),\n",
       " 96: array([ 96, 183, 140]),\n",
       " 97: array([ 97, 160, 105]),\n",
       " 98: array([ 98, 105, 182]),\n",
       " 99: array([ 99, 160, 105]),\n",
       " 100: array([100, 160, 105]),\n",
       " 101: array([101, 160, 105]),\n",
       " 102: array([102, 160, 130]),\n",
       " 103: array([103,  45, 112]),\n",
       " 104: array([104, 105, 160]),\n",
       " 105: array([105, 160, 166]),\n",
       " 106: array([106, 160, 105]),\n",
       " 107: array([107, 160, 105]),\n",
       " 108: array([108, 160, 105]),\n",
       " 109: array([109, 160, 105]),\n",
       " 110: array([110, 160, 105]),\n",
       " 111: array([111, 130, 160]),\n",
       " 112: array([112,  45, 160]),\n",
       " 113: array([113, 160, 130]),\n",
       " 114: array([114, 105, 160]),\n",
       " 115: array([115, 160, 105]),\n",
       " 116: array([116, 160, 105]),\n",
       " 117: array([117, 105, 160]),\n",
       " 118: array([118, 130, 160]),\n",
       " 119: array([119, 160, 105]),\n",
       " 120: array([120, 105, 160]),\n",
       " 121: array([121, 160, 105]),\n",
       " 122: array([122, 182, 105]),\n",
       " 123: array([123, 160, 105]),\n",
       " 124: array([124, 160, 130]),\n",
       " 125: array([125, 160, 105]),\n",
       " 126: array([126, 140,  83]),\n",
       " 127: array([127,  45, 130]),\n",
       " 128: array([128, 160, 105]),\n",
       " 129: array([129, 130,  45]),\n",
       " 130: array([130, 160, 105]),\n",
       " 131: array([131, 160, 105]),\n",
       " 132: array([132, 160, 105]),\n",
       " 133: array([133, 160, 105]),\n",
       " 134: array([134, 160, 105]),\n",
       " 135: array([135, 130, 160]),\n",
       " 136: array([136, 160, 105]),\n",
       " 137: array([137, 187, 105]),\n",
       " 138: array([138, 105, 160]),\n",
       " 139: array([139, 130, 160]),\n",
       " 140: array([140,  72,  45]),\n",
       " 141: array([141, 140, 183]),\n",
       " 142: array([142, 140, 160]),\n",
       " 143: array([143, 105, 160]),\n",
       " 144: array([144, 160, 105]),\n",
       " 145: array([145, 160, 105]),\n",
       " 146: array([146, 160, 105]),\n",
       " 147: array([147,  28, 105]),\n",
       " 148: array([148, 195,  30]),\n",
       " 149: array([149, 160, 105]),\n",
       " 150: array([150, 160, 105]),\n",
       " 151: array([151, 160, 105]),\n",
       " 152: array([152, 160, 105]),\n",
       " 153: array([153, 160, 105]),\n",
       " 154: array([154, 160, 105]),\n",
       " 155: array([155, 105, 160]),\n",
       " 156: array([156, 130, 112]),\n",
       " 157: array([157,  45, 160]),\n",
       " 158: array([158, 160, 105]),\n",
       " 159: array([159, 160, 105]),\n",
       " 160: array([160, 105, 130]),\n",
       " 161: array([161, 130, 160]),\n",
       " 162: array([162, 112, 130]),\n",
       " 163: array([163, 160, 105]),\n",
       " 164: array([164, 160, 105]),\n",
       " 165: array([165, 105, 160]),\n",
       " 166: array([166, 105, 160]),\n",
       " 167: array([167, 160, 105]),\n",
       " 168: array([168, 112,  45]),\n",
       " 169: array([169,  45, 160]),\n",
       " 170: array([170, 160, 105]),\n",
       " 171: array([171, 160, 105]),\n",
       " 172: array([172, 160, 130]),\n",
       " 173: array([173, 160, 105]),\n",
       " 174: array([174,  45, 112]),\n",
       " 175: array([175,  84,  45]),\n",
       " 176: array([176,  45, 112]),\n",
       " 177: array([177, 160, 105]),\n",
       " 178: array([178,  45, 112]),\n",
       " 179: array([179, 112,  45]),\n",
       " 180: array([180, 160, 105]),\n",
       " 181: array([181, 160, 105]),\n",
       " 182: array([182, 122, 160]),\n",
       " 183: array([183, 140,  72]),\n",
       " 184: array([184,  45, 112]),\n",
       " 185: array([185, 160, 105]),\n",
       " 186: array([186, 160, 105]),\n",
       " 187: array([187, 160, 105]),\n",
       " 188: array([188, 130, 112]),\n",
       " 189: array([189, 160, 105]),\n",
       " 190: array([190, 160, 105]),\n",
       " 191: array([191, 112,  45]),\n",
       " 192: array([192, 160, 105]),\n",
       " 193: array([193,  45, 112]),\n",
       " 194: array([194, 130, 160]),\n",
       " 195: array([195, 148,  30]),\n",
       " 196: array([196, 160, 105]),\n",
       " 197: array([197, 160, 130]),\n",
       " 198: array([198, 160, 105]),\n",
       " 199: array([199, 105, 160])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_mapping('stock', 'log_return_wap')\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4106234,
     "sourceId": 7143468,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 154007177,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 226.690282,
   "end_time": "2023-12-07T13:18:51.667438",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-07T13:15:04.977156",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
